[DEFAULT]
# Flags for defining the tf.train.ClusterSpec
# Comma-separated list of hostname:port pairs
ps_hosts = localhost:13337
worker_hosts = localhost:13338,localhost:13339
;worker_hosts = localhost:13338,localhost:13339,localhost:13340
;worker_hosts = localhost:13338,localhost:13339,localhost:13340,localhost:13341
;worker_hosts = localhost:13338,localhost:13339,localhost:13340,localhost:13341,localhost:13342

# Flags for the Q-learning hyperparameters
# Learning rate for Adam optimizer
learning_rate = 5e-4
# Size of the sample used for the mini-batch fed to every learning step
batch_size = 32
# Max size of the experience replay memory
memory_size = 50000
# Local timesteps between updates to the target Q-network
target_update = 1000

# Flags for FedAvg algorithm hyperparameters
# Seed for randomness (reproducibility)
seed = 1
# Total number of communication rounds to execute before interrupting
comm_rounds = 100000
# Number of epochs to run every communication round (from the start)
start_epoch = 100
# Number of epochs to run every communication round (after epoch_decay rounds)
end_epoch = 50
# Linearly decay number of epochs from start_epoch to end_epoch over epoch_decay rounds
epoch_decay = 1000
# Alpha startas at 1, this is its final value (after alpha_decay communication rouns)
end_alpha = 1
# Over how many communication rounds to decay alpha to end_alpha
alpha_decay = 3000
# Number of backup workers to use (implicitly sets n = total - b)
backup = 0

[async]
;learning_rate = 5e-3
;memory_size = 10000
target_update = 1000
start_epoch = 100
end_epoch = 100
epoch_decay = 500
;comm_rounds = 3000
comm_rounds = 30000000

[sync]
backup = 3
